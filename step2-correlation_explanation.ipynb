{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import corex_topic as ct\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read arXiv papers\n",
    "\n",
    "Note that these have already been collected via arXiv's OAI API, and text fields have been cleaned (lemmatized + nltk stopwords removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "top_dir = \"data/\"\n",
    "for file_name in os.listdir(top_dir):\n",
    "    if not (file_name.startswith(\"arxiv\") and file_name.endswith(\".json\")):\n",
    "        continue\n",
    "    _df = pd.read_json(top_dir+file_name,orient=\"records\")\n",
    "    condition = _df[\"categories\"].str.contains(\"stat.ML\") | _df[\"categories\"].str.contains(\"cs.\")\n",
    "    condition = condition & (_df.summary.apply(lambda s : len(s) > 20))\n",
    "    new_df = _df.loc[condition].copy()\n",
    "    data.append(new_df)\n",
    "    del (_df)\n",
    "df = pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253047"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling\n",
    "### Preprocessing: Generate a one-hot bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=0.001,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply basic settings, including ngram generation\n",
    "count_vectoriser = CountVectorizer(binary=True, min_df=0.001, ngram_range=(1,3))\n",
    "count_vectoriser.fit(map(lambda x : \" \".join(x), df.summary))\n",
    "\n",
    "# Generate vocab mapping\n",
    "X = count_vectoriser.transform(map(lambda x : \" \".join(x), df.summary))\n",
    "vocab = {v:k for k,v in count_vectoriser.vocabulary_.items()}\n",
    "print(\"Got vocab of size\",len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modelling using CorEx \n",
    "#### Finding the optimal number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out since process is slow, see results below\n",
    "# for i in range(10,50):\n",
    "#     topic_model = ct.Corex(n_hidden=i)\n",
    "#     topic_model.fit(X)\n",
    "#     print(i, topic_model.tc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The above process is slow, so I post my results here:\n",
    "\n",
    "n_hidden corex\n",
    "25 19.966864669267267\n",
    "26 20.149889848379193\n",
    "27 20.185964619605613\n",
    "28 20.785702813731525\n",
    "29 19.72899069238859\n",
    "30 20.745296242924486\n",
    "31 20.61876303771805\n",
    "32 20.392134766406286\n",
    "33 20.81017396572358\n",
    "34 20.572720324263855\n",
    "\n",
    "28 'appears to be' an inflection point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.138360113264966"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model = ct.Corex(n_hidden=28)\n",
    "topic_model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the topics, and generate topic weights for each arXiv paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: 'words' not provided to CorEx. Returning topics as lists of column indices\n"
     ]
    }
   ],
   "source": [
    "topics = topic_model.get_topics()\n",
    "# Build topic names\n",
    "topic_names = [\"_\".join(vocab[i] for i,_ in topic) for topic in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy_electron_wave_field_magnetic_particle_temperature_surface_atom_ion  \n",
      "\n",
      "learning_neural_neural network_state art_art_training_image_task_deep_classification  \n",
      "\n",
      "optical_laser_beam_light_photon_pulse_detector_frequency_resonance_wavelength  \n",
      "\n",
      "channel_wireless_transmission_receiver_interference_communication_transmitter_transmit_mimo_antenna  \n",
      "\n",
      "polynomial_prove_np_polynomial time_log_known_given_np hard_number_bounded  \n",
      "\n",
      "algorithm_problem_bound_optimal_optimization_lower bound_upper bound_optimization problem_complexity_upper  \n",
      "\n",
      "network_world_social_real world_user_service_real_node_social network_resource  \n",
      "\n",
      "research_software_development_year_science_web_challenge_tool_scientific_application  \n",
      "\n",
      "estimation_sparse_matrix_bayesian_likelihood_error_estimator_markov_convergence_estimate  \n",
      "\n",
      "dynamic_interaction_evolution_observed_dynamical_law_force_behavior_phenomenon_formation  \n",
      "\n",
      "system_design_technology_device_high_control_sensor_mobile_environment_hardware  \n",
      "\n",
      "quantum_equation_matter_mechanic_theory_einstein_spin_gravitational_classical_relativistic  \n",
      "\n",
      "propose_approach_proposed_novel_based_paper propose_existing_propose new_propose novel_effectiveness  \n",
      "\n",
      "language_logic_semantics_word_natural language_semantic_answer_text_sentence_program  \n",
      "\n",
      "code_coding_decoding_security_secure_ad hoc_hoc_bit_decoder_ad  \n",
      "\n",
      "distribution_monte_carlo_monte carlo_density_analytical_regime_parameter_transition_probability  \n",
      "\n",
      "performance_data_significantly_better_improve_technique_cost_efficient_computational_improvement  \n",
      "\n",
      "graph_vertex_edge_neighbor_random walk_directed_undirected_nearest neighbor_walk_subgraph  \n",
      "\n",
      "ha_however_many_often_important_make_still_attention_may_recently  \n",
      "\n",
      "model_large scale_scale_large_widely used_widely_model based_prediction_modeling_proposed model  \n",
      "\n",
      "simulation_numerical_numerical simulation_simulation result_numerical result_closed form_noise ratio_signal noise_closed_signal noise ratio  \n",
      "\n",
      "method_proposed method_method based_new method_based method_method used_method proposed_show method_method ha_differential equation  \n",
      "\n",
      "paper_paper present_paper study_paper show_paper consider_paper describes_paper introduce_paper address_paper investigate_paper focus  \n",
      "\n",
      "role_play_order magnitude_magnitude_important role_play important_play important role_order_pave way_pave  \n",
      "\n",
      "two_first_second_two different_first order_first time_second order_one_third_three  \n",
      "\n",
      "show_result show_also show_show proposed_result_experiment show_experimental result show_analysis show_result show proposed_finally show \n",
      "\n",
      "work_framework_information_knowledge_step_previous work_previous_prior_key_amount  \n",
      "\n",
      "empirical_theoretic_clustering_information theoretic_mutual information_mutual_cluster_privacy_utility_clustering algorithm  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in topic_names:\n",
    "    print(t,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_dir = \"../data/\"\n",
    "# ifile = 0\n",
    "# nfiles = len(os.listdir(top_dir))\n",
    "# all_dfs = []\n",
    "# # Iterate over files in the data dir\n",
    "# for file_name in os.listdir(top_dir):    \n",
    "#     if not (file_name.startswith(\"arxiv\") and file_name.endswith(\".json\")):\n",
    "#         continue\n",
    "#     ifile += 1    \n",
    "#     print(\"\\r\",ifile,\"   (\",ifile/nfiles,\")               \",end=\"\")\n",
    "#     x_df = pd.read_json(top_dir+file_name,orient=\"records\")\n",
    "#     # Filter out relevant arXiv categories\n",
    "#     condition = x_df.categories.apply(lambda x : any(y.startswith(\"cs.\") or y == \"stat.ML\"\n",
    "#                                                     for y in x.split()))\n",
    "#     # Require at least 20 words per summary\n",
    "#     condition = condition & (x_df.summary.apply(lambda s : len(s) > 20))\n",
    "#     # Make a copy of the subset\n",
    "#     _df = x_df.loc[condition].copy()\n",
    "#     _df.reset_index(inplace=True)\n",
    "#     del x_df\n",
    "#     # Bag up the words and generate the topic weights\n",
    "#     X_all = count_vectoriser.transform(map(lambda x : \" \".join(x), _df.summary))    \n",
    "#     topic_weights = defaultdict(list)\n",
    "#     for t,_t in zip(topic_names,topics):\n",
    "#         # Weight is given by the the sum of weights for the topic in this summary\n",
    "#         for bow,(irow,row) in zip(X_all,_df.iterrows()):\n",
    "#             total_weight = 0\n",
    "#             for idx, weight in _t:                \n",
    "#                 total_weight += bow[0,idx]*weight\n",
    "#             topic_weights[t].append(total_weight)\n",
    "#     # Assign the weights to the DF\n",
    "#     for t,w in topic_weights.items():\n",
    "#         _df[t] = w\n",
    "#     all_dfs.append(_df)\n",
    "\n",
    "# # Bring it all together\n",
    "# _df = pd.concat(all_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag up the words and generate the topic weights\n",
    "topic_weights = defaultdict(list)\n",
    "df_corex = df.copy()\n",
    "for t, _t in zip(topic_names, topics):\n",
    "    # Weight is given by the the sum of weights for the topic in this summary\n",
    "    for bow, (irow, row) in zip(X, df_corex.iterrows()):\n",
    "        total_weight = 0\n",
    "        for idx, weight in _t:\n",
    "            total_weight += bow[0, idx]*weight\n",
    "        topic_weights[t].append(total_weight)\n",
    "# Assign the weights to the DF\n",
    "for t, w in topic_weights.items():\n",
    "    df_corex[t] = w\n",
    "df_corex.to_json(\"arxiv_corex_with_weights.json\",orient=\"records\")\n",
    "del df_corex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253047"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corex = pd.read_json(\"arxiv_corex_with_weights.json\",orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modelling using LDA \n",
    "#### Finding the optimal number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #corpus = [[(idx, 1) for idx in row.indices] for row in X]\n",
    "# gendict = Dictionary(df.summary.values)\n",
    "# corpus = [gendict.doc2bow(text) for text in df.summary.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to preprocess\n",
    "texts = []\n",
    "for row in X:\n",
    "    sentence = [vocab[w] for w in row.indices]\n",
    "    texts.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gendict = Dictionary(texts)\n",
    "corpus = [gendict.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 -4.992124503469249\n",
      "21 -5.059349407257665\n"
     ]
    }
   ],
   "source": [
    "for n in np.arange(20,30,1):\n",
    "    lda_model = LdaMulticore(corpus, num_topics=n, id2word=gendict, \n",
    "                             iterations=1000, chunksize=20000)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, corpus=corpus, \n",
    "                                         texts=texts, coherence='c_uci')\n",
    "    print(n, coherence_model_lda.get_coherence())\n",
    "    del lda_model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The above process is slow, so I post my results here:\n",
    "\n",
    "n_hidden UCI\n",
    "20 -0.07677146689828407\n",
    "21 -0.06265104443822114\n",
    "22 -0.06445968901965661\n",
    "23 -0.047409571995300916\n",
    "24 -0.0897110576799478\n",
    "25 -0.0634228910962744\n",
    "26 -0.005620293211399472\n",
    "27 -0.060972405381606636\n",
    "28 -0.0659826487685569\n",
    "29 -0.018131262824714105\n",
    "\n",
    "26 'appears to be' minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaMulticore(corpus, num_topics=26, id2word=gendict, \n",
    "                         iterations=1000, chunksize=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_network_algorithm_graph_time_based_method_problem_approach_system\n",
      "model_energy_state_scale_time_two_flow_system_number_show\n",
      "algorithm_problem_method_image_data_learning_based_system_model_paper\n",
      "time_algorithm_n_method_problem_two_result_space_set_show\n",
      "model_result_system_bound_show_problem_two_one_field_also\n",
      "system_time_quantum_model_state_based_show_paper_equation_approach\n",
      "g_k_method_graph_n_two_network_result_algorithm_state\n",
      "model_data_method_based_learning_problem_show_system_approach_using\n",
      "field_model_time_state_energy_two_quantum_effect_wave_system\n",
      "method_model_equation_two_system_field_result_problem_using_based\n",
      "time_two_system_frequency_result_field_energy_electron_model_using\n",
      "method_energy_result_x_using_based_model_high_detector_ha\n",
      "system_model_based_user_paper_two_approach_algorithm_using_time\n",
      "mode_model_optical_laser_wave_field_high_frequency_beam_energy\n",
      "network_data_model_learning_neural_based_feature_deep_image_result\n",
      "algorithm_problem_result_time_number_network_model_optimal_show_two\n",
      "n_system_k_time_model_result_two_data_show_number\n",
      "data_surface_method_problem_paper_using_approach_system_channel_result\n",
      "network_quantum_model_system_agent_based_state_show_using_paper\n",
      "model_method_matrix_result_approach_show_based_problem_one_using\n",
      "network_channel_scheme_rate_node_result_paper_energy_proposed_information\n",
      "system_user_paper_model_application_data_based_design_ha_approach\n",
      "structure_model_result_time_logic_system_show_based_ha_data\n",
      "code_word_information_show_also_paper_error_algorithm_based_problem\n",
      "image_time_system_based_model_flow_using_show_state_approach\n",
      "n_problem_graph_k_log_bound_p_result_show_algorithm\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,26):\n",
    "    print(\"_\".join(gendict[idx] for idx,_  in lda_model.get_topic_terms(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaMulticore(corpus, num_topics=28, id2word=gendict, \n",
    "                         iterations=100, chunksize=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_time_show_also_model_one_state_two_using_study\n",
      "problem_show_algorithm_time_paper_graph_result_two_also_given\n",
      "network_data_method_neural_neural network_based_model_state_paper_using\n",
      "paper_problem_system_show_result_based_algorithm_performance_time_also\n",
      "model_result_study_show_also_time_structure_based_system_two\n",
      "result_show_simulation_paper_model_using_study_also_network_numerical\n",
      "model_two_system_dynamic_different_state_convolutional neural network_convolutional neural_based_result\n",
      "network_paper_show_model_ha_method_structure_system_result_new\n",
      "present_paper_data_time_based_model_system_new_one_ha\n",
      "network_paper_based_method_show_approach_system_result_propose_data\n",
      "based_proposed_result_performance_show_using_algorithm_paper_method_model\n",
      "high_using_optical_system_result_state_based_used_field_ha\n",
      "paper_show_model_learning_result_time_based_problem_ha_used\n",
      "algorithm_result_method_data_based_show_problem_paper_two_new\n",
      "network_model_show_paper_information_problem_learning_also_result_two\n",
      "model_equation_result_field_energy_two_show_using_also_method\n",
      "show_result_also_two_time_problem_paper_one_new_number\n",
      "paper_show_using_result_present_based_system_method_time_problem\n",
      "system_result_show_based_two_optical_also_using_energy_mode\n",
      "problem_result_paper_also_two_using_algorithm_ha_system_show\n",
      "time_show_based_used_two_one_result_paper_using_method\n",
      "paper_system_ha_data_using_based_present_result_approach_model\n",
      "result_paper_based_model_method_using_image_present_high_proposed\n",
      "also_method_one_based_paper_time_result_system_structure_ha\n",
      "based_using_result_study_model_also_two_method_approach_work\n",
      "field_result_time_two_present_paper_method_ha_using_also\n",
      "using_data_method_time_two_based_result_experiment_model_show\n",
      "system_result_time_based_model_paper_approach_two_also_one\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,28):\n",
    "    print(\"_\".join(gendict[idx] for idx,_  in lda_model.get_topic_terms(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm',\n",
       " 'algorithmic',\n",
       " 'also',\n",
       " 'also present',\n",
       " 'appear',\n",
       " 'attention',\n",
       " 'attention recent',\n",
       " 'based',\n",
       " 'characterization',\n",
       " 'color',\n",
       " 'colored',\n",
       " 'concerning',\n",
       " 'connection',\n",
       " 'decomposition',\n",
       " 'describe',\n",
       " 'describe new',\n",
       " 'ell',\n",
       " 'expose',\n",
       " 'family',\n",
       " 'game',\n",
       " 'generalize',\n",
       " 'give',\n",
       " 'give new',\n",
       " 'graph',\n",
       " 'graph algorithm',\n",
       " 'increased',\n",
       " 'instance',\n",
       " 'lee',\n",
       " 'nash',\n",
       " 'new',\n",
       " 'new algorithm',\n",
       " 'obtain',\n",
       " 'particular',\n",
       " 'present',\n",
       " 'present new',\n",
       " 'previous',\n",
       " 'previous result',\n",
       " 'problem',\n",
       " 'proof',\n",
       " 'received',\n",
       " 'recent',\n",
       " 'recent year',\n",
       " 'result',\n",
       " 'rigidity',\n",
       " 'solution',\n",
       " 'sparse',\n",
       " 'sparsity',\n",
       " 'special',\n",
       " 'strengthen',\n",
       " 'theory',\n",
       " 'tree',\n",
       " 'use',\n",
       " 'work',\n",
       " 'work also',\n",
       " 'year']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
